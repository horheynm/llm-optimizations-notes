{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b7b68d-0ec3-4682-bfce-266e0e2a02af",
   "metadata": {},
   "source": [
    "# PTQ\n",
    "\n",
    "Post training quantization (PTQ) is the process of reducing the precision of model weights for an already-trained model, such as pretrained model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83caf571-d81b-4c16-95fd-e51ae6dbc443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimizing Quantization Error\n",
    "\n",
    "# TODO\n",
    "# calibration, how it works, (zero shot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c685efc7-e167-41f8-b39a-bdf08549b247",
   "metadata": {},
   "source": [
    "## PTQ Impact\n",
    "\n",
    "### Faster Inference on Supported Machines\n",
    "\n",
    "Quantization can significantly improve inference speed on hardware that supports lower-precision computations. Many modern GPUs and accelerators, such as NVIDIA's H100 GPUs, support specialized formats like `fp8`. When these formats are natively supported, the hardware can process computations directly, resulting in faster inference.\n",
    "\n",
    "However, on hardware that does not support the target format, the quantized representation must first be converted to a supported precision (usually higher precision, `fp16`, `fp32` etc.) before computations can occur. This conversion introduces overhead, which can negate the performance gains of quantization. Therefore, quantization is most effective when used with hardware designed for lower-precision arithmetic.\n",
    "\n",
    "> PTQ + supported hardware -> faster inference\n",
    "\n",
    "\n",
    "### Faster Inference Using Specialized Kernels\n",
    "\n",
    "To leverage the hardware computation for LLM workloads - matrix multiplications - specialized kernels can be integrated for a specific precision. Kernels are computer instructions that provide details on data manipulation and computation (ex. apply weight shuffling to align weight entries for memory access patterns for NVIDIA tensor cores to maximize throughput and minimize loading latency). One example is [Machete kernel](https://neuralmagic.com/blog/introducing-machete-a-mixed-input-gemm-kernel-optimized-for-nvidia-hopper-gpus/?utm_source=chatgpt.com), specifically optimized for H100 GPUs and model weights quantized to 4 bits. Another is Marlin, which contain instructions for A100 GPUs for `int4` weights and `fp16` activations.\n",
    "\n",
    "\n",
    "To maximize the computational potential of modern hardware for LLM workloads — primarily matrix multiplications — specialized kernels are employed. Kernels are sets of computer instructions designed to optimize how data is manipulated and computations are performed. Ex. techniques like weight shuffling reorganizes weight data to align with memory access patterns, allowing NVIDIA Tensor Cores to maximize throughput and minimize loading latency. Specialized Kernels include:\n",
    "\n",
    "* **Machete**:\n",
    "Optimized for NVIDIA H100 GPUs. Designed for models with weights quantized to `int4`. By leveraging hardware features like advanced Tensor Core instructions (wgmma) and efficient memory access strategies, Machete dramatically improves inference speed and reduces memory usage - mediam time to first token to <250ms and median time per output token to <100ms with 1xH100 on Llama 3.1-70B.\n",
    "\n",
    "* **Marlin**:\n",
    "Tailored for NVIDIA A100 GPUs, Marlin supports mixed-precision computations with INT4 weights and FP16 activations. It provides optimized instructions to handle these lower-precision formats efficiently, enabling faster inference on hardware that supports these precision formats.\n",
    "\n",
    "> PTQ + supported hardware + specific kernel -> much faster inference\n",
    "\n",
    "Using quantization techniques, along with precison supported hardware and specialized kernels, it demonstrates how hardware and software innovations work together to handle the growing computational demands of LLMs, achieving faster inference times while using less memory.`m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a753d7c-1b0d-4802-9fb3-23fed550cf7d",
   "metadata": {},
   "source": [
    "# Which algo is supported on whihc kernel - here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e90518-6d47-4a50-bf4e-6fa6c819d8e5",
   "metadata": {},
   "source": [
    "# How do the kernels work - kernel.ipyb\n",
    "# # What are the impact of using the kernel - kernel.ipyb\n",
    "# In what situations do you use what - ptq_algorithms.ipyb\n",
    "- in low QPS, use weight only - not much speed up in high QPS\n",
    "- in high QPS or offline, use activation quantization\n",
    "      A100 - w8a8 chan weights, dynamic per token act\n",
    "      H100 - w8a8 fp8 chan weight, dpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fba615-d775-4047-812c-15b7cad190cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
